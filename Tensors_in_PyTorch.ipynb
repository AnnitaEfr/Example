{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"Sigmoid activation function.\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data.\n",
    "torch.manual_seed(7)  # Set the random seed so things are predictable.\n",
    "\n",
    "# Features are 5 random normal variables.\n",
    "\n",
    "features = torch.randn((1, 5)) #creates a tensor with shape (1, 5), one row and five columns,\n",
    "#that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one.\n",
    "\n",
    "# True weights for our data, random normal variables again.\n",
    "weights = torch.randn_like(features) #creates another tensor with the same shape as features, again containing values from a normal distribution.\n",
    "\n",
    "\n",
    "# and a true bias term.\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "**Calculate the output of the network with input features, weights, and bias. Similar to Numpy, PyTorch has a torch.sum() function, as well as a .sum() method on tensors, for taking sums. Use the function activation defined above as the activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors.\n",
    "\n",
    "features = torch.randn(1, 5)\n",
    "weights = torch.randn(1, 5)\n",
    "\n",
    "print(features.shape)\n",
    "print(weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4382],\n",
       "        [0.2028],\n",
       "        [2.4505],\n",
       "        [2.0256],\n",
       "        [1.7792]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.view(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.4820]])\n"
     ]
    }
   ],
   "source": [
    "output = torch.mm(features, weights.T) + bias  # matrix multiplication\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data.\n",
    "torch.manual_seed(7)  # Set the random seed so things are predictable.\n",
    "\n",
    "# Features are 3 random normal variables.\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network.\n",
    "n_input = features.shape[1]  # Number of input units, must match number of input features.\n",
    "n_hidden = 2  # Number of hidden units.\n",
    "n_output = 1  # Number of output units.\n",
    "\n",
    "# Weights for inputs to hidden layer.\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer.\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers.\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Calculate the output for this multi-layer network using the weights W1 & W2, and the biases, B1 & B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[-0.1468,  0.7861,  0.9468]])\n",
      "Hidden layer activations: tensor([[0.6813, 0.4355]])\n",
      "Final output: tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "# Define the activation function (Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# Step 1: Calculate hidden layer activations\n",
    "hidden_layer = torch.matmul(features, W1) + B1  # Linear combination (shape: (1, 2))\n",
    "hidden_layer_activation = sigmoid(hidden_layer)  # Apply activation (shape: (1, 2))\n",
    "\n",
    "# Step 2: Calculate output layer\n",
    "output_layer = torch.matmul(hidden_layer_activation, W2) + B2  # Linear combination (shape: (1, 1))\n",
    "final_output = sigmoid(output_layer)  # Apply activation (optional, shape: (1, 1))\n",
    "\n",
    "# Print results\n",
    "print(\"Features:\", features)\n",
    "print(\"Hidden layer activations:\", hidden_layer_activation)\n",
    "print(\"Final output:\", final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
